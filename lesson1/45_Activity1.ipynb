{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"accidentes\"\n",
    "master = \"local[*]\"\n",
    "spark = (SparkSession.builder\n",
    "    .master(master)\n",
    "    .config(\"spark.driver.cores\", 1)\n",
    "    .appName(app_name)\n",
    "    .getOrCreate() )\n",
    "sc = spark.sparkContext\n",
    "print ('SparkContext created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def getLinesFromUrl(url:str):\n",
    "    \"\"\"\n",
    "    Downloads content from a url and\n",
    "    creates a text based RDD\n",
    "    \"\"\"\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read().decode('utf-8')\n",
    "    lines = data.split('\\n')\n",
    "    # creates a RDD for the book\n",
    "    return sc.parallelize(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huckleberry_lines = getLinesFromUrl ('https://www.gutenberg.org/files/76/76-0.txt')\n",
    "hamlet_lines = getLinesFromUrl ('https://www.gutenberg.org/files/2265/2265.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def getRDDWords(book):\n",
    "    def clean_line(line):\n",
    "        \"\"\"\n",
    "        Remove \\ufeff\\r characters\n",
    "        Remove \\t \\n \\r\n",
    "        Remove additional characters\n",
    "        \"\"\"\n",
    "        return line.replace('\\ufeff\\r', '').\\\n",
    "            replace('\\t', ' ').replace('\\n', '').replace('\\r', '').\\\n",
    "            replace('(', '').replace(')', '').replace(\"'\", '').\\\n",
    "            replace('\"', '').replace(',', ''). replace('.', '').\\\n",
    "            replace('*', '')\n",
    "\n",
    "    def normalize_tokenize(line):\n",
    "        \"\"\"\n",
    "        Normalize: lowercase\n",
    "        tokenize: split in tokens\n",
    "        \"\"\"\n",
    "        return re.sub('\\s+', ' ', line).strip().lower().split(' ')\n",
    "\n",
    "    return book.map(lambda x: clean_line(x)).\\\n",
    "        filter(lambda x: x != '').flatMap(normalize_tokenize) #.\\\n",
    "        #filter(lambda x: len(x)>3).\\\n",
    "        #map (lambda x: (x, 1)).\\\n",
    "        #reduceByKey (lambda x, y: x + y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huckleberry_words = getRDDWords (huckleberry_lines)\n",
    "huckleberry_counts = huckleberry_words.map (lambda x: (x, 1)).\\\n",
    "                                       reduceByKey (lambda x, y: x + y)\n",
    "print ('Tokens: {} / First {}'.format (huckleberry_counts.count(), huckleberry_counts.take(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_words = getRDDWords (hamlet_lines)\n",
    "hamlet_counts = hamlet_words.map (lambda x: (x, 1)).\\\n",
    "                                       reduceByKey (lambda x, y: x + y)\n",
    "print ('Tokens: {} / First {}'.format (hamlet_counts.count(), hamlet_counts.take(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcular la media de la longitud de palabra\n",
    "---\n",
    "<br>\n",
    "puede hacer uso de la funcion [RDD].reduce(f)\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desviación estándar\n",
    "---\n",
    "$\\Large\\text{DE} = \\sqrt{\\dfrac{\\sum\\limits_{}^{}{{\\lvert x-\\mu\\rvert^2}}}{N}}$\n",
    "\n",
    "### donde: \n",
    "* $\\text{x}$ son los elementos de la población, cada una de las longitudes\n",
    "* $\\mu$ es la media de la población, en este caso la longitud media de las palabras\n",
    "* $\\text{N}$ es el número de elementos de la población, la cantidad de palabras\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 palabras más frecuentes\n",
    "---\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimina el contexto spark\n",
    "#Debe ser la última sentencia\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
